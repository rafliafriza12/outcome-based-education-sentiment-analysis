{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from CSV files\n",
    "def load_data_from_csv(manual_file_path, gpt_file_path):\n",
    "    # Load CSV files\n",
    "    manual_df = pd.read_csv(manual_file_path)\n",
    "    gpt_df = pd.read_csv(gpt_file_path)\n",
    "    \n",
    "    # Check that both dataframes have the same texts\n",
    "    if not manual_df['full_text'].equals(gpt_df['full_text']):\n",
    "        print(\"Warning: The texts in the two CSV files don't match exactly.\")\n",
    "        \n",
    "    return manual_df, gpt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_file_path = \"./labeled_data/labeled_sentiment_manual_annotation.csv\" \n",
    "gpt_file_path = \"./labeled_data/labeled_sentiment_gpt_for_cohens_kappa.csv\" \n",
    "    \n",
    "# Load data from CSV files\n",
    "manual_df, gpt_df = load_data_from_csv(manual_file_path, gpt_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cohen's Kappa\n",
    "def calculate_cohen_kappa(manual_df, gpt_df):\n",
    "    # Extract the labels (annotations)\n",
    "    manual_annotations = manual_df['label'].tolist()\n",
    "    gpt_annotations = gpt_df['label'].tolist()\n",
    "    \n",
    "    # Calculate Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(manual_annotations, gpt_annotations)\n",
    "    \n",
    "    return kappa, manual_annotations, gpt_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret Cohen's Kappa\n",
    "def interpret_kappa(kappa):\n",
    "    if kappa < 0:\n",
    "        return \"Poor agreement (less than chance)\"\n",
    "    elif kappa < 0.2:\n",
    "        return \"Slight agreement\"\n",
    "    elif kappa < 0.4:\n",
    "        return \"Fair agreement\"\n",
    "    elif kappa < 0.6:\n",
    "        return \"Moderate agreement\"\n",
    "    elif kappa < 0.8:\n",
    "        return \"Substantial agreement\"\n",
    "    else:\n",
    "        return \"Almost perfect agreement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.8154\n",
      "Kappa Interpretation: Almost perfect agreement\n"
     ]
    }
   ],
   "source": [
    "kappa, manual_annotations, gpt_annotations = calculate_cohen_kappa(manual_df, gpt_df)\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(f\"Kappa Interpretation: {interpret_kappa(kappa)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "def create_confusion_matrix(y_true, y_pred):\n",
    "    labels = sorted(list(set(y_true + y_pred)))\n",
    "    n_labels = len(labels)\n",
    "    \n",
    "    # Create a dictionary mapping label to index\n",
    "    label_to_idx = {label: i for i, label in enumerate(labels)}\n",
    "    \n",
    "    # Initialize confusion matrix\n",
    "    cm = np.zeros((n_labels, n_labels), dtype=int)\n",
    "    \n",
    "    # Fill confusion matrix\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        cm[label_to_idx[true], label_to_idx[pred]] += 1\n",
    "    \n",
    "    return cm, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "Labels: ['negatif', 'netral', 'positif']\n",
      "[[53  1  2]\n",
      " [ 3  5  0]\n",
      " [ 3  1 32]]\n"
     ]
    }
   ],
   "source": [
    "cm, labels = create_confusion_matrix(manual_annotations, gpt_annotations)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"Labels:\", labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate agreement metrics per class\n",
    "def per_class_metrics(cm, labels):\n",
    "    n_labels = len(labels)\n",
    "    metrics = {}\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        true_pos = cm[i, i]\n",
    "        false_pos = sum(cm[:, i]) - true_pos\n",
    "        false_neg = sum(cm[i, :]) - true_pos\n",
    "        \n",
    "        # Count instances where both annotators agree this is not the class\n",
    "        true_neg = np.sum(cm) - true_pos - false_pos - false_neg\n",
    "        \n",
    "        # Calculate agreement percentage for this class\n",
    "        total = np.sum(cm)\n",
    "        agreement = (true_pos + true_neg) / total\n",
    "        \n",
    "        metrics[label] = {\n",
    "            \"agreement\": agreement,\n",
    "            \"true_positive\": true_pos,\n",
    "            \"false_positive\": false_pos,\n",
    "            \"false_negative\": false_neg,\n",
    "            \"true_negative\": true_neg\n",
    "        }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Class Metrics:\n",
      "\n",
      "Class: negatif\n",
      "Agreement: 0.9100\n",
      "True Positives: 53\n",
      "False Positives: 6\n",
      "False Negatives: 3\n",
      "True Negatives: 38\n",
      "\n",
      "Class: netral\n",
      "Agreement: 0.9500\n",
      "True Positives: 5\n",
      "False Positives: 2\n",
      "False Negatives: 3\n",
      "True Negatives: 90\n",
      "\n",
      "Class: positif\n",
      "Agreement: 0.9400\n",
      "True Positives: 32\n",
      "False Positives: 2\n",
      "False Negatives: 4\n",
      "True Negatives: 62\n",
      "\n",
      "Overall Percentage Agreement: 90.00%\n"
     ]
    }
   ],
   "source": [
    " # Calculate and display per-class metrics\n",
    "class_metrics = per_class_metrics(cm, labels)\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for label, metrics in class_metrics.items():\n",
    "    print(f\"\\nClass: {label}\")\n",
    "    print(f\"Agreement: {metrics['agreement']:.4f}\")\n",
    "    print(f\"True Positives: {metrics['true_positive']}\")\n",
    "    print(f\"False Positives: {metrics['false_positive']}\")\n",
    "    print(f\"False Negatives: {metrics['false_negative']}\")\n",
    "    print(f\"True Negatives: {metrics['true_negative']}\")\n",
    "    \n",
    "# Calculate overall percentage agreement\n",
    "correct = sum(1 for m, g in zip(manual_annotations, gpt_annotations) if m == g)\n",
    "total = len(manual_annotations)\n",
    "percentage_agreement = correct / total * 100\n",
    "print(f\"\\nOverall Percentage Agreement: {percentage_agreement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of Manual Annotations:\n",
      "Counter({'negatif': 56, 'positif': 36, 'netral': 8})\n",
      "\n",
      "Distribution of GPT-4o-mini Annotations:\n",
      "Counter({'negatif': 59, 'positif': 34, 'netral': 7})\n"
     ]
    }
   ],
   "source": [
    "# Display distribution of annotations\n",
    "print(\"\\nDistribution of Manual Annotations:\")\n",
    "print(Counter(manual_annotations))\n",
    "print(\"\\nDistribution of GPT-4o-mini Annotations:\")\n",
    "print(Counter(gpt_annotations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
